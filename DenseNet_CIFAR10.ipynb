{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suPWWhI7w9f_"
      },
      "source": [
        "- Copyright 2021. Dongwon Kim All rights reserved.\n",
        "- File name : DenseNet_CIFAR10.ipynb\n",
        "- Written by Dongwon Kim\n",
        "- DenseNet\n",
        "    - build an advanced CNN model to classify CIFAR-10\n",
        "- Modificatoin history\n",
        "    - written by Dongwon Kim on Oct 10, 2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVT39mj9wzV1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from google.colab import files\n",
        "from torchsummary import summary\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyEpgbiSERaw"
      },
      "source": [
        "# Choosing Model to use\n",
        "- first tried with VGG16, but accuracy was not enough\n",
        "- try DenseNet instead refering the [rank](https://paperswithcode.com/sota/image-classification-on-cifar-10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewnCsUaaF1DQ"
      },
      "source": [
        "# Parameters\n",
        "- batch size: 128\n",
        "- epoch: 200\n",
        "- learning rate: 0.1\n",
        "- dropout: 0.2\n",
        "- growth rate: 12\n",
        "- reduction: 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLJAJIn3xUSH"
      },
      "source": [
        "# Prepare dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjnNlbRWFiUQ"
      },
      "source": [
        "## Download Data\n",
        "1. load CIFAR10 dataset from torchvision.datasets\n",
        "2. change dataset to tenser and normalize with meand and std  \n",
        "[reference](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
        "    - image: 3 x 32 x32\n",
        "    - total 10 classes\n",
        "3. download train and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEPULc0YxEh7",
        "outputId": "a1223713-4e71-49c5-9a80-c410f6153803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root = './',\n",
        "    download = True,\n",
        "    train = True,\n",
        "    transform = transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root = './',\n",
        "    download = True,\n",
        "    train = False,\n",
        "    transform = transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkemwvfrycZA",
        "outputId": "af12ca5e-6fb7-4c97-8aa3-68397e00f9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3) 50000\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.data.shape, len(train_dataset.targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZAuAGufzj5x",
        "outputId": "8125ac87-2e6f-4063-95c5-72d002c2107c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 32, 32, 3) 10000\n"
          ]
        }
      ],
      "source": [
        "print(test_dataset.data.shape, len(test_dataset.targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEZQhH7WFfDH"
      },
      "source": [
        "## Shuffle and Split train, validation data\n",
        "1. by using train_test_split, shuffle the dataset and split data\n",
        "    - validation size = 10% of train data\n",
        "    - split data to have similar ratio of targets using stratify option  \n",
        "2. set samplers for each dataset using SubsetRandomSampler "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "022eR7r8xkNA"
      },
      "outputs": [],
      "source": [
        "tr_index, val_index = train_test_split(list(range(len(train_dataset))), test_size = 0.1, shuffle=True, stratify = train_dataset.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTrTuoAsxlGK"
      },
      "outputs": [],
      "source": [
        "tr_sampler= SubsetRandomSampler(tr_index)\n",
        "val_sampler = SubsetRandomSampler(val_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB70bLBOFvLB"
      },
      "source": [
        "## Set DataLoader\n",
        "1. using the samplers, set train, val loader\n",
        "2. since test dataset is not shuffled and splited, no need to use a sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8Vnd6gF0a8W"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    sampler = tr_sampler\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    sampler = val_sampler\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YWdxXRCGBHj"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuFydYc4GEhU"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzu75SFM0t06"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ata8RVyZGIB6"
      },
      "source": [
        "## Bottleneck\n",
        "- use bottleneck block instead of normal dense block\n",
        "- to reduce feature map insert 1x1 conv layer before 3x3 conv layer\n",
        "- also increase computational efficiency\n",
        "- reference\n",
        "    - [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993v5.pdf) paper\n",
        "\n",
        "> BatchNorm(BN) → relu → 1x1 Conv → BN → relu → 3x3 Conv\n",
        "\n",
        "- for 1x1 Conv refering the paper, produce 4 * growth rate \n",
        "- add Dropout layer to prevent overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMNrRTXt9NFv"
      },
      "outputs": [],
      "source": [
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_plane, growth_rate, droprate):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_plane)\n",
        "        self.conv1 = nn.Conv2d(in_plane, 4*growth_rate, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
        "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size = 3, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.droprate = droprate\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)        \n",
        "\n",
        "        return torch.cat([out, x], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK1bceuUHnSN"
      },
      "source": [
        "## TransitionBlock\n",
        "- in DenseNet, use transition block for **compression**\n",
        "- therefore, use 2 x 2 average pooling\n",
        "\n",
        "> BN → relu → 1 x 1 Conv → 2 x 2 Avg Pooling\n",
        "\n",
        "- add Dropout layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GPI9F1yByAD"
      },
      "outputs": [],
      "source": [
        "class TransitionBlock(nn.Module):\n",
        "    def __init__(self, in_plane, out_plane, droprate):\n",
        "        super(TransitionBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_plane)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_plane, out_plane, kernel_size=1, stride=1, bias=False)\n",
        "        self.droprate = droprate\n",
        "       \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = F.dropout(out, p=self.droprate, training=self.training, inplace=False)\n",
        "        \n",
        "        return F.avg_pool2d(out, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HriNzTDyI2N7"
      },
      "source": [
        "## DenseNet\n",
        "- make DenseNet using Bottleneck and transition block\n",
        "- refering the paper, \n",
        "    - set θ as 0.5, growth rate as 12\n",
        "    - set 1st 3 x 3 Conv layer's output channels as twice the growth rate\n",
        "    - for Dense121, \n",
        "        - no. of Dense block: 6 12 24 16 \n",
        "\n",
        "> conv → dense1 → transition1 → dense2 → transition2 → dense3 → transition3 → dense4 → classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWMZaMu1CwHS"
      },
      "outputs": [],
      "source": [
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, droprate, block=BottleneckBlock, growth_rate=12, num_classes=10, reduction=0.5):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        \n",
        "\n",
        "        in_plane = 2 * growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, in_plane, kernel_size = 3, padding=1, bias=False)\n",
        "\n",
        "        # 1st Dense & Transition\n",
        "        self.dense1 = self.make_dense_block(block, in_plane, 6, droprate)\n",
        "        in_plane += 6 * growth_rate\n",
        "        out_plane = int(math.floor(in_plane * reduction))\n",
        "        self.trans1 = TransitionBlock(in_plane, out_plane, droprate)\n",
        "        in_plane = out_plane\n",
        "\n",
        "        # 2nd Dense & Transition\n",
        "        self.dense2 = self.make_dense_block(block, in_plane, 12,droprate)\n",
        "        in_plane += 12 * growth_rate\n",
        "        out_plane = int(math.floor(in_plane * reduction))\n",
        "        self.trans2 = TransitionBlock(in_plane, out_plane, droprate)\n",
        "        in_plane = out_plane\n",
        "\n",
        "        # 3rd Dense & Transition\n",
        "        self.dense3 = self.make_dense_block(block, in_plane, 24, droprate)\n",
        "        in_plane += 24 * growth_rate\n",
        "        out_plane = int(math.floor(in_plane * reduction))\n",
        "        self.trans3 = TransitionBlock(in_plane, out_plane, droprate)\n",
        "        in_plane = out_plane\n",
        "\n",
        "        # 4th Dense\n",
        "        self.dense4 = self.make_dense_block(block, in_plane, 16, droprate)\n",
        "        in_plane += 16 * growth_rate\n",
        "        \n",
        "        self.bn = nn.BatchNorm2d(in_plane)\n",
        "        self.fc = nn.Linear(in_plane, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. /n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "       \n",
        "\n",
        "    def make_dense_block(self, block, in_plane, nblock, droprate):\n",
        "        layers=[]\n",
        "        for i in range(nblock):\n",
        "            layers.append(block(in_plane, self.growth_rate, droprate))\n",
        "            in_plane += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 32 x 32\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        # 32 x 32\n",
        "        out = self.dense1(out)\n",
        "        out = self.trans1(out) # 32 -> 16\n",
        "        \n",
        "        # 16 x 16\n",
        "        out = self.dense2(out)\n",
        "        out = self.trans2(out) # 16 -> 8\n",
        "        \n",
        "        # 8 x 8\n",
        "        out = self.dense3(out)\n",
        "        out = self.trans3(out) # 8 -> 4\n",
        "\n",
        "        out = self.dense4(out)\n",
        "\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HYIla_ZMQ32"
      },
      "outputs": [],
      "source": [
        "model = DenseNet(droprate=0.2)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMK5LWzbQPMQ",
        "outputId": "ac63d242-7bc5-4432-f219-4536d78a7cb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (dense1): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (trans1): TransitionBlock(\n",
              "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  )\n",
              "  (dense2): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (trans2): TransitionBlock(\n",
              "    (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  )\n",
              "  (dense3): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (14): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (15): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (16): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (17): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(300, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (18): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(312, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (19): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(324, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (20): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(336, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (21): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(348, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(348, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (22): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(360, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (23): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(372, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (trans3): TransitionBlock(\n",
              "    (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  )\n",
              "  (dense4): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(300, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(312, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(324, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(336, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(348, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(348, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (14): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(360, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (15): BottleneckBlock(\n",
              "      (bn1): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(372, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc): Linear(in_features=384, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOG0d9CBNRR_",
        "outputId": "938061a2-4ce0-4a80-ec0e-c8905e1b87d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 24, 32, 32]             648\n",
            "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
            "              ReLU-3           [-1, 24, 32, 32]               0\n",
            "            Conv2d-4           [-1, 48, 32, 32]           1,152\n",
            "       BatchNorm2d-5           [-1, 48, 32, 32]              96\n",
            "              ReLU-6           [-1, 48, 32, 32]               0\n",
            "            Conv2d-7           [-1, 12, 32, 32]           5,184\n",
            "   BottleneckBlock-8           [-1, 36, 32, 32]               0\n",
            "       BatchNorm2d-9           [-1, 36, 32, 32]              72\n",
            "             ReLU-10           [-1, 36, 32, 32]               0\n",
            "           Conv2d-11           [-1, 48, 32, 32]           1,728\n",
            "      BatchNorm2d-12           [-1, 48, 32, 32]              96\n",
            "             ReLU-13           [-1, 48, 32, 32]               0\n",
            "           Conv2d-14           [-1, 12, 32, 32]           5,184\n",
            "  BottleneckBlock-15           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-16           [-1, 48, 32, 32]              96\n",
            "             ReLU-17           [-1, 48, 32, 32]               0\n",
            "           Conv2d-18           [-1, 48, 32, 32]           2,304\n",
            "      BatchNorm2d-19           [-1, 48, 32, 32]              96\n",
            "             ReLU-20           [-1, 48, 32, 32]               0\n",
            "           Conv2d-21           [-1, 12, 32, 32]           5,184\n",
            "  BottleneckBlock-22           [-1, 60, 32, 32]               0\n",
            "      BatchNorm2d-23           [-1, 60, 32, 32]             120\n",
            "             ReLU-24           [-1, 60, 32, 32]               0\n",
            "           Conv2d-25           [-1, 48, 32, 32]           2,880\n",
            "      BatchNorm2d-26           [-1, 48, 32, 32]              96\n",
            "             ReLU-27           [-1, 48, 32, 32]               0\n",
            "           Conv2d-28           [-1, 12, 32, 32]           5,184\n",
            "  BottleneckBlock-29           [-1, 72, 32, 32]               0\n",
            "      BatchNorm2d-30           [-1, 72, 32, 32]             144\n",
            "             ReLU-31           [-1, 72, 32, 32]               0\n",
            "           Conv2d-32           [-1, 48, 32, 32]           3,456\n",
            "      BatchNorm2d-33           [-1, 48, 32, 32]              96\n",
            "             ReLU-34           [-1, 48, 32, 32]               0\n",
            "           Conv2d-35           [-1, 12, 32, 32]           5,184\n",
            "  BottleneckBlock-36           [-1, 84, 32, 32]               0\n",
            "      BatchNorm2d-37           [-1, 84, 32, 32]             168\n",
            "             ReLU-38           [-1, 84, 32, 32]               0\n",
            "           Conv2d-39           [-1, 48, 32, 32]           4,032\n",
            "      BatchNorm2d-40           [-1, 48, 32, 32]              96\n",
            "             ReLU-41           [-1, 48, 32, 32]               0\n",
            "           Conv2d-42           [-1, 12, 32, 32]           5,184\n",
            "  BottleneckBlock-43           [-1, 96, 32, 32]               0\n",
            "      BatchNorm2d-44           [-1, 96, 32, 32]             192\n",
            "             ReLU-45           [-1, 96, 32, 32]               0\n",
            "           Conv2d-46           [-1, 48, 32, 32]           4,608\n",
            "  TransitionBlock-47           [-1, 48, 16, 16]               0\n",
            "      BatchNorm2d-48           [-1, 48, 16, 16]              96\n",
            "             ReLU-49           [-1, 48, 16, 16]               0\n",
            "           Conv2d-50           [-1, 48, 16, 16]           2,304\n",
            "      BatchNorm2d-51           [-1, 48, 16, 16]              96\n",
            "             ReLU-52           [-1, 48, 16, 16]               0\n",
            "           Conv2d-53           [-1, 12, 16, 16]           5,184\n",
            "  BottleneckBlock-54           [-1, 60, 16, 16]               0\n",
            "      BatchNorm2d-55           [-1, 60, 16, 16]             120\n",
            "             ReLU-56           [-1, 60, 16, 16]               0\n",
            "           Conv2d-57           [-1, 48, 16, 16]           2,880\n",
            "      BatchNorm2d-58           [-1, 48, 16, 16]              96\n",
            "             ReLU-59           [-1, 48, 16, 16]               0\n",
            "           Conv2d-60           [-1, 12, 16, 16]           5,184\n",
            "  BottleneckBlock-61           [-1, 72, 16, 16]               0\n",
            "      BatchNorm2d-62           [-1, 72, 16, 16]             144\n",
            "             ReLU-63           [-1, 72, 16, 16]               0\n",
            "           Conv2d-64           [-1, 48, 16, 16]           3,456\n",
            "      BatchNorm2d-65           [-1, 48, 16, 16]              96\n",
            "             ReLU-66           [-1, 48, 16, 16]               0\n",
            "           Conv2d-67           [-1, 12, 16, 16]           5,184\n",
            "  BottleneckBlock-68           [-1, 84, 16, 16]               0\n",
            "      BatchNorm2d-69           [-1, 84, 16, 16]             168\n",
            "             ReLU-70           [-1, 84, 16, 16]               0\n",
            "           Conv2d-71           [-1, 48, 16, 16]           4,032\n",
            "      BatchNorm2d-72           [-1, 48, 16, 16]              96\n",
            "             ReLU-73           [-1, 48, 16, 16]               0\n",
            "           Conv2d-74           [-1, 12, 16, 16]           5,184\n",
            "  BottleneckBlock-75           [-1, 96, 16, 16]               0\n",
            "      BatchNorm2d-76           [-1, 96, 16, 16]             192\n",
            "             ReLU-77           [-1, 96, 16, 16]               0\n",
            "           Conv2d-78           [-1, 48, 16, 16]           4,608\n",
            "      BatchNorm2d-79           [-1, 48, 16, 16]              96\n",
            "             ReLU-80           [-1, 48, 16, 16]               0\n",
            "           Conv2d-81           [-1, 12, 16, 16]           5,184\n",
            "  BottleneckBlock-82          [-1, 108, 16, 16]               0\n",
            "      BatchNorm2d-83          [-1, 108, 16, 16]             216\n",
            "             ReLU-84          [-1, 108, 16, 16]               0\n",
            "           Conv2d-85           [-1, 48, 16, 16]           5,184\n",
            "      BatchNorm2d-86           [-1, 48, 16, 16]              96\n",
            "             ReLU-87           [-1, 48, 16, 16]               0\n",
            "           Conv2d-88           [-1, 12, 16, 16]           5,184\n",
            "  BottleneckBlock-89          [-1, 120, 16, 16]               0\n",
            "      BatchNorm2d-90          [-1, 120, 16, 16]             240\n",
            "             ReLU-91          [-1, 120, 16, 16]               0\n",
            "           Conv2d-92           [-1, 48, 16, 16]           5,760\n",
            "      BatchNorm2d-93           [-1, 48, 16, 16]              96\n",
            "             ReLU-94           [-1, 48, 16, 16]               0\n",
            "           Conv2d-95           [-1, 12, 16, 16]           5,184\n",
            "  BottleneckBlock-96          [-1, 132, 16, 16]               0\n",
            "      BatchNorm2d-97          [-1, 132, 16, 16]             264\n",
            "             ReLU-98          [-1, 132, 16, 16]               0\n",
            "           Conv2d-99           [-1, 48, 16, 16]           6,336\n",
            "     BatchNorm2d-100           [-1, 48, 16, 16]              96\n",
            "            ReLU-101           [-1, 48, 16, 16]               0\n",
            "          Conv2d-102           [-1, 12, 16, 16]           5,184\n",
            " BottleneckBlock-103          [-1, 144, 16, 16]               0\n",
            "     BatchNorm2d-104          [-1, 144, 16, 16]             288\n",
            "            ReLU-105          [-1, 144, 16, 16]               0\n",
            "          Conv2d-106           [-1, 48, 16, 16]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 16, 16]              96\n",
            "            ReLU-108           [-1, 48, 16, 16]               0\n",
            "          Conv2d-109           [-1, 12, 16, 16]           5,184\n",
            " BottleneckBlock-110          [-1, 156, 16, 16]               0\n",
            "     BatchNorm2d-111          [-1, 156, 16, 16]             312\n",
            "            ReLU-112          [-1, 156, 16, 16]               0\n",
            "          Conv2d-113           [-1, 48, 16, 16]           7,488\n",
            "     BatchNorm2d-114           [-1, 48, 16, 16]              96\n",
            "            ReLU-115           [-1, 48, 16, 16]               0\n",
            "          Conv2d-116           [-1, 12, 16, 16]           5,184\n",
            " BottleneckBlock-117          [-1, 168, 16, 16]               0\n",
            "     BatchNorm2d-118          [-1, 168, 16, 16]             336\n",
            "            ReLU-119          [-1, 168, 16, 16]               0\n",
            "          Conv2d-120           [-1, 48, 16, 16]           8,064\n",
            "     BatchNorm2d-121           [-1, 48, 16, 16]              96\n",
            "            ReLU-122           [-1, 48, 16, 16]               0\n",
            "          Conv2d-123           [-1, 12, 16, 16]           5,184\n",
            " BottleneckBlock-124          [-1, 180, 16, 16]               0\n",
            "     BatchNorm2d-125          [-1, 180, 16, 16]             360\n",
            "            ReLU-126          [-1, 180, 16, 16]               0\n",
            "          Conv2d-127           [-1, 48, 16, 16]           8,640\n",
            "     BatchNorm2d-128           [-1, 48, 16, 16]              96\n",
            "            ReLU-129           [-1, 48, 16, 16]               0\n",
            "          Conv2d-130           [-1, 12, 16, 16]           5,184\n",
            " BottleneckBlock-131          [-1, 192, 16, 16]               0\n",
            "     BatchNorm2d-132          [-1, 192, 16, 16]             384\n",
            "            ReLU-133          [-1, 192, 16, 16]               0\n",
            "          Conv2d-134           [-1, 96, 16, 16]          18,432\n",
            " TransitionBlock-135             [-1, 96, 8, 8]               0\n",
            "     BatchNorm2d-136             [-1, 96, 8, 8]             192\n",
            "            ReLU-137             [-1, 96, 8, 8]               0\n",
            "          Conv2d-138             [-1, 48, 8, 8]           4,608\n",
            "     BatchNorm2d-139             [-1, 48, 8, 8]              96\n",
            "            ReLU-140             [-1, 48, 8, 8]               0\n",
            "          Conv2d-141             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-142            [-1, 108, 8, 8]               0\n",
            "     BatchNorm2d-143            [-1, 108, 8, 8]             216\n",
            "            ReLU-144            [-1, 108, 8, 8]               0\n",
            "          Conv2d-145             [-1, 48, 8, 8]           5,184\n",
            "     BatchNorm2d-146             [-1, 48, 8, 8]              96\n",
            "            ReLU-147             [-1, 48, 8, 8]               0\n",
            "          Conv2d-148             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-149            [-1, 120, 8, 8]               0\n",
            "     BatchNorm2d-150            [-1, 120, 8, 8]             240\n",
            "            ReLU-151            [-1, 120, 8, 8]               0\n",
            "          Conv2d-152             [-1, 48, 8, 8]           5,760\n",
            "     BatchNorm2d-153             [-1, 48, 8, 8]              96\n",
            "            ReLU-154             [-1, 48, 8, 8]               0\n",
            "          Conv2d-155             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-156            [-1, 132, 8, 8]               0\n",
            "     BatchNorm2d-157            [-1, 132, 8, 8]             264\n",
            "            ReLU-158            [-1, 132, 8, 8]               0\n",
            "          Conv2d-159             [-1, 48, 8, 8]           6,336\n",
            "     BatchNorm2d-160             [-1, 48, 8, 8]              96\n",
            "            ReLU-161             [-1, 48, 8, 8]               0\n",
            "          Conv2d-162             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-163            [-1, 144, 8, 8]               0\n",
            "     BatchNorm2d-164            [-1, 144, 8, 8]             288\n",
            "            ReLU-165            [-1, 144, 8, 8]               0\n",
            "          Conv2d-166             [-1, 48, 8, 8]           6,912\n",
            "     BatchNorm2d-167             [-1, 48, 8, 8]              96\n",
            "            ReLU-168             [-1, 48, 8, 8]               0\n",
            "          Conv2d-169             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-170            [-1, 156, 8, 8]               0\n",
            "     BatchNorm2d-171            [-1, 156, 8, 8]             312\n",
            "            ReLU-172            [-1, 156, 8, 8]               0\n",
            "          Conv2d-173             [-1, 48, 8, 8]           7,488\n",
            "     BatchNorm2d-174             [-1, 48, 8, 8]              96\n",
            "            ReLU-175             [-1, 48, 8, 8]               0\n",
            "          Conv2d-176             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-177            [-1, 168, 8, 8]               0\n",
            "     BatchNorm2d-178            [-1, 168, 8, 8]             336\n",
            "            ReLU-179            [-1, 168, 8, 8]               0\n",
            "          Conv2d-180             [-1, 48, 8, 8]           8,064\n",
            "     BatchNorm2d-181             [-1, 48, 8, 8]              96\n",
            "            ReLU-182             [-1, 48, 8, 8]               0\n",
            "          Conv2d-183             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-184            [-1, 180, 8, 8]               0\n",
            "     BatchNorm2d-185            [-1, 180, 8, 8]             360\n",
            "            ReLU-186            [-1, 180, 8, 8]               0\n",
            "          Conv2d-187             [-1, 48, 8, 8]           8,640\n",
            "     BatchNorm2d-188             [-1, 48, 8, 8]              96\n",
            "            ReLU-189             [-1, 48, 8, 8]               0\n",
            "          Conv2d-190             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-191            [-1, 192, 8, 8]               0\n",
            "     BatchNorm2d-192            [-1, 192, 8, 8]             384\n",
            "            ReLU-193            [-1, 192, 8, 8]               0\n",
            "          Conv2d-194             [-1, 48, 8, 8]           9,216\n",
            "     BatchNorm2d-195             [-1, 48, 8, 8]              96\n",
            "            ReLU-196             [-1, 48, 8, 8]               0\n",
            "          Conv2d-197             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-198            [-1, 204, 8, 8]               0\n",
            "     BatchNorm2d-199            [-1, 204, 8, 8]             408\n",
            "            ReLU-200            [-1, 204, 8, 8]               0\n",
            "          Conv2d-201             [-1, 48, 8, 8]           9,792\n",
            "     BatchNorm2d-202             [-1, 48, 8, 8]              96\n",
            "            ReLU-203             [-1, 48, 8, 8]               0\n",
            "          Conv2d-204             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-205            [-1, 216, 8, 8]               0\n",
            "     BatchNorm2d-206            [-1, 216, 8, 8]             432\n",
            "            ReLU-207            [-1, 216, 8, 8]               0\n",
            "          Conv2d-208             [-1, 48, 8, 8]          10,368\n",
            "     BatchNorm2d-209             [-1, 48, 8, 8]              96\n",
            "            ReLU-210             [-1, 48, 8, 8]               0\n",
            "          Conv2d-211             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-212            [-1, 228, 8, 8]               0\n",
            "     BatchNorm2d-213            [-1, 228, 8, 8]             456\n",
            "            ReLU-214            [-1, 228, 8, 8]               0\n",
            "          Conv2d-215             [-1, 48, 8, 8]          10,944\n",
            "     BatchNorm2d-216             [-1, 48, 8, 8]              96\n",
            "            ReLU-217             [-1, 48, 8, 8]               0\n",
            "          Conv2d-218             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-219            [-1, 240, 8, 8]               0\n",
            "     BatchNorm2d-220            [-1, 240, 8, 8]             480\n",
            "            ReLU-221            [-1, 240, 8, 8]               0\n",
            "          Conv2d-222             [-1, 48, 8, 8]          11,520\n",
            "     BatchNorm2d-223             [-1, 48, 8, 8]              96\n",
            "            ReLU-224             [-1, 48, 8, 8]               0\n",
            "          Conv2d-225             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-226            [-1, 252, 8, 8]               0\n",
            "     BatchNorm2d-227            [-1, 252, 8, 8]             504\n",
            "            ReLU-228            [-1, 252, 8, 8]               0\n",
            "          Conv2d-229             [-1, 48, 8, 8]          12,096\n",
            "     BatchNorm2d-230             [-1, 48, 8, 8]              96\n",
            "            ReLU-231             [-1, 48, 8, 8]               0\n",
            "          Conv2d-232             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-233            [-1, 264, 8, 8]               0\n",
            "     BatchNorm2d-234            [-1, 264, 8, 8]             528\n",
            "            ReLU-235            [-1, 264, 8, 8]               0\n",
            "          Conv2d-236             [-1, 48, 8, 8]          12,672\n",
            "     BatchNorm2d-237             [-1, 48, 8, 8]              96\n",
            "            ReLU-238             [-1, 48, 8, 8]               0\n",
            "          Conv2d-239             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-240            [-1, 276, 8, 8]               0\n",
            "     BatchNorm2d-241            [-1, 276, 8, 8]             552\n",
            "            ReLU-242            [-1, 276, 8, 8]               0\n",
            "          Conv2d-243             [-1, 48, 8, 8]          13,248\n",
            "     BatchNorm2d-244             [-1, 48, 8, 8]              96\n",
            "            ReLU-245             [-1, 48, 8, 8]               0\n",
            "          Conv2d-246             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-247            [-1, 288, 8, 8]               0\n",
            "     BatchNorm2d-248            [-1, 288, 8, 8]             576\n",
            "            ReLU-249            [-1, 288, 8, 8]               0\n",
            "          Conv2d-250             [-1, 48, 8, 8]          13,824\n",
            "     BatchNorm2d-251             [-1, 48, 8, 8]              96\n",
            "            ReLU-252             [-1, 48, 8, 8]               0\n",
            "          Conv2d-253             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-254            [-1, 300, 8, 8]               0\n",
            "     BatchNorm2d-255            [-1, 300, 8, 8]             600\n",
            "            ReLU-256            [-1, 300, 8, 8]               0\n",
            "          Conv2d-257             [-1, 48, 8, 8]          14,400\n",
            "     BatchNorm2d-258             [-1, 48, 8, 8]              96\n",
            "            ReLU-259             [-1, 48, 8, 8]               0\n",
            "          Conv2d-260             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-261            [-1, 312, 8, 8]               0\n",
            "     BatchNorm2d-262            [-1, 312, 8, 8]             624\n",
            "            ReLU-263            [-1, 312, 8, 8]               0\n",
            "          Conv2d-264             [-1, 48, 8, 8]          14,976\n",
            "     BatchNorm2d-265             [-1, 48, 8, 8]              96\n",
            "            ReLU-266             [-1, 48, 8, 8]               0\n",
            "          Conv2d-267             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-268            [-1, 324, 8, 8]               0\n",
            "     BatchNorm2d-269            [-1, 324, 8, 8]             648\n",
            "            ReLU-270            [-1, 324, 8, 8]               0\n",
            "          Conv2d-271             [-1, 48, 8, 8]          15,552\n",
            "     BatchNorm2d-272             [-1, 48, 8, 8]              96\n",
            "            ReLU-273             [-1, 48, 8, 8]               0\n",
            "          Conv2d-274             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-275            [-1, 336, 8, 8]               0\n",
            "     BatchNorm2d-276            [-1, 336, 8, 8]             672\n",
            "            ReLU-277            [-1, 336, 8, 8]               0\n",
            "          Conv2d-278             [-1, 48, 8, 8]          16,128\n",
            "     BatchNorm2d-279             [-1, 48, 8, 8]              96\n",
            "            ReLU-280             [-1, 48, 8, 8]               0\n",
            "          Conv2d-281             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-282            [-1, 348, 8, 8]               0\n",
            "     BatchNorm2d-283            [-1, 348, 8, 8]             696\n",
            "            ReLU-284            [-1, 348, 8, 8]               0\n",
            "          Conv2d-285             [-1, 48, 8, 8]          16,704\n",
            "     BatchNorm2d-286             [-1, 48, 8, 8]              96\n",
            "            ReLU-287             [-1, 48, 8, 8]               0\n",
            "          Conv2d-288             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-289            [-1, 360, 8, 8]               0\n",
            "     BatchNorm2d-290            [-1, 360, 8, 8]             720\n",
            "            ReLU-291            [-1, 360, 8, 8]               0\n",
            "          Conv2d-292             [-1, 48, 8, 8]          17,280\n",
            "     BatchNorm2d-293             [-1, 48, 8, 8]              96\n",
            "            ReLU-294             [-1, 48, 8, 8]               0\n",
            "          Conv2d-295             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-296            [-1, 372, 8, 8]               0\n",
            "     BatchNorm2d-297            [-1, 372, 8, 8]             744\n",
            "            ReLU-298            [-1, 372, 8, 8]               0\n",
            "          Conv2d-299             [-1, 48, 8, 8]          17,856\n",
            "     BatchNorm2d-300             [-1, 48, 8, 8]              96\n",
            "            ReLU-301             [-1, 48, 8, 8]               0\n",
            "          Conv2d-302             [-1, 12, 8, 8]           5,184\n",
            " BottleneckBlock-303            [-1, 384, 8, 8]               0\n",
            "     BatchNorm2d-304            [-1, 384, 8, 8]             768\n",
            "            ReLU-305            [-1, 384, 8, 8]               0\n",
            "          Conv2d-306            [-1, 192, 8, 8]          73,728\n",
            " TransitionBlock-307            [-1, 192, 4, 4]               0\n",
            "     BatchNorm2d-308            [-1, 192, 4, 4]             384\n",
            "            ReLU-309            [-1, 192, 4, 4]               0\n",
            "          Conv2d-310             [-1, 48, 4, 4]           9,216\n",
            "     BatchNorm2d-311             [-1, 48, 4, 4]              96\n",
            "            ReLU-312             [-1, 48, 4, 4]               0\n",
            "          Conv2d-313             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-314            [-1, 204, 4, 4]               0\n",
            "     BatchNorm2d-315            [-1, 204, 4, 4]             408\n",
            "            ReLU-316            [-1, 204, 4, 4]               0\n",
            "          Conv2d-317             [-1, 48, 4, 4]           9,792\n",
            "     BatchNorm2d-318             [-1, 48, 4, 4]              96\n",
            "            ReLU-319             [-1, 48, 4, 4]               0\n",
            "          Conv2d-320             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-321            [-1, 216, 4, 4]               0\n",
            "     BatchNorm2d-322            [-1, 216, 4, 4]             432\n",
            "            ReLU-323            [-1, 216, 4, 4]               0\n",
            "          Conv2d-324             [-1, 48, 4, 4]          10,368\n",
            "     BatchNorm2d-325             [-1, 48, 4, 4]              96\n",
            "            ReLU-326             [-1, 48, 4, 4]               0\n",
            "          Conv2d-327             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-328            [-1, 228, 4, 4]               0\n",
            "     BatchNorm2d-329            [-1, 228, 4, 4]             456\n",
            "            ReLU-330            [-1, 228, 4, 4]               0\n",
            "          Conv2d-331             [-1, 48, 4, 4]          10,944\n",
            "     BatchNorm2d-332             [-1, 48, 4, 4]              96\n",
            "            ReLU-333             [-1, 48, 4, 4]               0\n",
            "          Conv2d-334             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-335            [-1, 240, 4, 4]               0\n",
            "     BatchNorm2d-336            [-1, 240, 4, 4]             480\n",
            "            ReLU-337            [-1, 240, 4, 4]               0\n",
            "          Conv2d-338             [-1, 48, 4, 4]          11,520\n",
            "     BatchNorm2d-339             [-1, 48, 4, 4]              96\n",
            "            ReLU-340             [-1, 48, 4, 4]               0\n",
            "          Conv2d-341             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-342            [-1, 252, 4, 4]               0\n",
            "     BatchNorm2d-343            [-1, 252, 4, 4]             504\n",
            "            ReLU-344            [-1, 252, 4, 4]               0\n",
            "          Conv2d-345             [-1, 48, 4, 4]          12,096\n",
            "     BatchNorm2d-346             [-1, 48, 4, 4]              96\n",
            "            ReLU-347             [-1, 48, 4, 4]               0\n",
            "          Conv2d-348             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-349            [-1, 264, 4, 4]               0\n",
            "     BatchNorm2d-350            [-1, 264, 4, 4]             528\n",
            "            ReLU-351            [-1, 264, 4, 4]               0\n",
            "          Conv2d-352             [-1, 48, 4, 4]          12,672\n",
            "     BatchNorm2d-353             [-1, 48, 4, 4]              96\n",
            "            ReLU-354             [-1, 48, 4, 4]               0\n",
            "          Conv2d-355             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-356            [-1, 276, 4, 4]               0\n",
            "     BatchNorm2d-357            [-1, 276, 4, 4]             552\n",
            "            ReLU-358            [-1, 276, 4, 4]               0\n",
            "          Conv2d-359             [-1, 48, 4, 4]          13,248\n",
            "     BatchNorm2d-360             [-1, 48, 4, 4]              96\n",
            "            ReLU-361             [-1, 48, 4, 4]               0\n",
            "          Conv2d-362             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-363            [-1, 288, 4, 4]               0\n",
            "     BatchNorm2d-364            [-1, 288, 4, 4]             576\n",
            "            ReLU-365            [-1, 288, 4, 4]               0\n",
            "          Conv2d-366             [-1, 48, 4, 4]          13,824\n",
            "     BatchNorm2d-367             [-1, 48, 4, 4]              96\n",
            "            ReLU-368             [-1, 48, 4, 4]               0\n",
            "          Conv2d-369             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-370            [-1, 300, 4, 4]               0\n",
            "     BatchNorm2d-371            [-1, 300, 4, 4]             600\n",
            "            ReLU-372            [-1, 300, 4, 4]               0\n",
            "          Conv2d-373             [-1, 48, 4, 4]          14,400\n",
            "     BatchNorm2d-374             [-1, 48, 4, 4]              96\n",
            "            ReLU-375             [-1, 48, 4, 4]               0\n",
            "          Conv2d-376             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-377            [-1, 312, 4, 4]               0\n",
            "     BatchNorm2d-378            [-1, 312, 4, 4]             624\n",
            "            ReLU-379            [-1, 312, 4, 4]               0\n",
            "          Conv2d-380             [-1, 48, 4, 4]          14,976\n",
            "     BatchNorm2d-381             [-1, 48, 4, 4]              96\n",
            "            ReLU-382             [-1, 48, 4, 4]               0\n",
            "          Conv2d-383             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-384            [-1, 324, 4, 4]               0\n",
            "     BatchNorm2d-385            [-1, 324, 4, 4]             648\n",
            "            ReLU-386            [-1, 324, 4, 4]               0\n",
            "          Conv2d-387             [-1, 48, 4, 4]          15,552\n",
            "     BatchNorm2d-388             [-1, 48, 4, 4]              96\n",
            "            ReLU-389             [-1, 48, 4, 4]               0\n",
            "          Conv2d-390             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-391            [-1, 336, 4, 4]               0\n",
            "     BatchNorm2d-392            [-1, 336, 4, 4]             672\n",
            "            ReLU-393            [-1, 336, 4, 4]               0\n",
            "          Conv2d-394             [-1, 48, 4, 4]          16,128\n",
            "     BatchNorm2d-395             [-1, 48, 4, 4]              96\n",
            "            ReLU-396             [-1, 48, 4, 4]               0\n",
            "          Conv2d-397             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-398            [-1, 348, 4, 4]               0\n",
            "     BatchNorm2d-399            [-1, 348, 4, 4]             696\n",
            "            ReLU-400            [-1, 348, 4, 4]               0\n",
            "          Conv2d-401             [-1, 48, 4, 4]          16,704\n",
            "     BatchNorm2d-402             [-1, 48, 4, 4]              96\n",
            "            ReLU-403             [-1, 48, 4, 4]               0\n",
            "          Conv2d-404             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-405            [-1, 360, 4, 4]               0\n",
            "     BatchNorm2d-406            [-1, 360, 4, 4]             720\n",
            "            ReLU-407            [-1, 360, 4, 4]               0\n",
            "          Conv2d-408             [-1, 48, 4, 4]          17,280\n",
            "     BatchNorm2d-409             [-1, 48, 4, 4]              96\n",
            "            ReLU-410             [-1, 48, 4, 4]               0\n",
            "          Conv2d-411             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-412            [-1, 372, 4, 4]               0\n",
            "     BatchNorm2d-413            [-1, 372, 4, 4]             744\n",
            "            ReLU-414            [-1, 372, 4, 4]               0\n",
            "          Conv2d-415             [-1, 48, 4, 4]          17,856\n",
            "     BatchNorm2d-416             [-1, 48, 4, 4]              96\n",
            "            ReLU-417             [-1, 48, 4, 4]               0\n",
            "          Conv2d-418             [-1, 12, 4, 4]           5,184\n",
            " BottleneckBlock-419            [-1, 384, 4, 4]               0\n",
            "     BatchNorm2d-420            [-1, 384, 4, 4]             768\n",
            "            ReLU-421            [-1, 384, 4, 4]               0\n",
            "          Linear-422                   [-1, 10]           3,850\n",
            "================================================================\n",
            "Total params: 1,000,618\n",
            "Trainable params: 1,000,618\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 43.32\n",
            "Params size (MB): 3.82\n",
            "Estimated Total Size (MB): 47.15\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(model, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rZYbj-JKa2F"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmlEhZwbKioe"
      },
      "source": [
        "## Loss function and optimizer\n",
        "- refering the paper\n",
        "    - use SGD\n",
        "    - learning rate: 0.1, weight decay: 10^(-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGgpGhJ5NZSC"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec5MNSZ_nSRv"
      },
      "outputs": [],
      "source": [
        "train_batches = len(train_loader)\n",
        "val_batches = len(val_loader)\n",
        "\n",
        "best_valid_loss = 1024\n",
        "patience = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkbzQOJVLB8p"
      },
      "source": [
        "## Train\n",
        "- set epoch to 200\n",
        "    - although the paper set epoch 300 for CIFAR10, by experiment, because of overfitting, set to 200 \n",
        "    - train accuracy becomes 1 after 185 epoch and loss of validation set doesn't decrease\n",
        "- not using early stopping to train the model enough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l-LNYOHMIkb",
        "outputId": "dc676377-4ef5-470b-a24d-ff5ec070f188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/200] TrainLoss: 1.681, ValLoss: 1.710 | TrainAcc: 0.37, ValAcc: 0.41\n",
            "[2/200] TrainLoss: 1.235, ValLoss: 1.408 | TrainAcc: 0.56, ValAcc: 0.52\n",
            "[3/200] TrainLoss: 0.998, ValLoss: 1.089 | TrainAcc: 0.64, ValAcc: 0.61\n",
            "[4/200] TrainLoss: 0.876, ValLoss: 0.977 | TrainAcc: 0.69, ValAcc: 0.66\n",
            "[5/200] TrainLoss: 0.791, ValLoss: 0.972 | TrainAcc: 0.72, ValAcc: 0.67\n",
            "[6/200] TrainLoss: 0.725, ValLoss: 0.935 | TrainAcc: 0.74, ValAcc: 0.69\n",
            "[7/200] TrainLoss: 0.673, ValLoss: 0.732 | TrainAcc: 0.76, ValAcc: 0.74\n",
            "[8/200] TrainLoss: 0.633, ValLoss: 0.718 | TrainAcc: 0.78, ValAcc: 0.77\n",
            "[9/200] TrainLoss: 0.610, ValLoss: 0.651 | TrainAcc: 0.79, ValAcc: 0.78\n",
            "[10/200] TrainLoss: 0.589, ValLoss: 0.794 | TrainAcc: 0.80, ValAcc: 0.73\n",
            "[11/200] TrainLoss: 0.567, ValLoss: 0.652 | TrainAcc: 0.80, ValAcc: 0.77\n",
            "[12/200] TrainLoss: 0.555, ValLoss: 0.759 | TrainAcc: 0.81, ValAcc: 0.75\n",
            "[13/200] TrainLoss: 0.542, ValLoss: 0.832 | TrainAcc: 0.81, ValAcc: 0.74\n",
            "[14/200] TrainLoss: 0.536, ValLoss: 0.634 | TrainAcc: 0.81, ValAcc: 0.79\n",
            "[15/200] TrainLoss: 0.526, ValLoss: 0.575 | TrainAcc: 0.82, ValAcc: 0.80\n",
            "[16/200] TrainLoss: 0.520, ValLoss: 0.833 | TrainAcc: 0.82, ValAcc: 0.74\n",
            "[17/200] TrainLoss: 0.507, ValLoss: 0.569 | TrainAcc: 0.83, ValAcc: 0.80\n",
            "[18/200] TrainLoss: 0.506, ValLoss: 0.574 | TrainAcc: 0.83, ValAcc: 0.80\n",
            "[19/200] TrainLoss: 0.498, ValLoss: 0.608 | TrainAcc: 0.83, ValAcc: 0.80\n",
            "[20/200] TrainLoss: 0.497, ValLoss: 0.562 | TrainAcc: 0.83, ValAcc: 0.80\n",
            "[21/200] TrainLoss: 0.480, ValLoss: 0.571 | TrainAcc: 0.84, ValAcc: 0.81\n",
            "[22/200] TrainLoss: 0.482, ValLoss: 0.624 | TrainAcc: 0.83, ValAcc: 0.79\n",
            "[23/200] TrainLoss: 0.476, ValLoss: 0.821 | TrainAcc: 0.84, ValAcc: 0.75\n",
            "[24/200] TrainLoss: 0.473, ValLoss: 0.581 | TrainAcc: 0.84, ValAcc: 0.81\n",
            "[25/200] TrainLoss: 0.468, ValLoss: 0.671 | TrainAcc: 0.84, ValAcc: 0.78\n",
            "[26/200] TrainLoss: 0.467, ValLoss: 0.633 | TrainAcc: 0.84, ValAcc: 0.80\n",
            "[27/200] TrainLoss: 0.462, ValLoss: 0.584 | TrainAcc: 0.84, ValAcc: 0.80\n",
            "[28/200] TrainLoss: 0.460, ValLoss: 0.623 | TrainAcc: 0.84, ValAcc: 0.79\n",
            "[29/200] TrainLoss: 0.458, ValLoss: 0.511 | TrainAcc: 0.84, ValAcc: 0.82\n",
            "[30/200] TrainLoss: 0.448, ValLoss: 0.606 | TrainAcc: 0.85, ValAcc: 0.80\n",
            "[31/200] TrainLoss: 0.449, ValLoss: 0.554 | TrainAcc: 0.85, ValAcc: 0.81\n",
            "[32/200] TrainLoss: 0.445, ValLoss: 0.646 | TrainAcc: 0.85, ValAcc: 0.78\n",
            "[33/200] TrainLoss: 0.439, ValLoss: 0.535 | TrainAcc: 0.85, ValAcc: 0.82\n",
            "[34/200] TrainLoss: 0.438, ValLoss: 0.519 | TrainAcc: 0.85, ValAcc: 0.83\n",
            "[35/200] TrainLoss: 0.443, ValLoss: 0.520 | TrainAcc: 0.85, ValAcc: 0.82\n",
            "[36/200] TrainLoss: 0.437, ValLoss: 0.561 | TrainAcc: 0.85, ValAcc: 0.81\n",
            "[37/200] TrainLoss: 0.429, ValLoss: 0.537 | TrainAcc: 0.85, ValAcc: 0.82\n",
            "[38/200] TrainLoss: 0.433, ValLoss: 0.508 | TrainAcc: 0.85, ValAcc: 0.83\n",
            "[39/200] TrainLoss: 0.427, ValLoss: 0.534 | TrainAcc: 0.85, ValAcc: 0.82\n",
            "[40/200] TrainLoss: 0.421, ValLoss: 0.628 | TrainAcc: 0.85, ValAcc: 0.80\n",
            "[41/200] TrainLoss: 0.424, ValLoss: 0.568 | TrainAcc: 0.85, ValAcc: 0.81\n",
            "[42/200] TrainLoss: 0.420, ValLoss: 0.471 | TrainAcc: 0.85, ValAcc: 0.83\n",
            "[43/200] TrainLoss: 0.415, ValLoss: 0.532 | TrainAcc: 0.86, ValAcc: 0.83\n",
            "[44/200] TrainLoss: 0.416, ValLoss: 0.493 | TrainAcc: 0.86, ValAcc: 0.83\n",
            "[45/200] TrainLoss: 0.421, ValLoss: 0.489 | TrainAcc: 0.85, ValAcc: 0.83\n",
            "[46/200] TrainLoss: 0.408, ValLoss: 0.455 | TrainAcc: 0.86, ValAcc: 0.84\n",
            "[47/200] TrainLoss: 0.409, ValLoss: 0.571 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[48/200] TrainLoss: 0.410, ValLoss: 0.620 | TrainAcc: 0.86, ValAcc: 0.79\n",
            "[49/200] TrainLoss: 0.407, ValLoss: 0.727 | TrainAcc: 0.86, ValAcc: 0.77\n",
            "[50/200] TrainLoss: 0.399, ValLoss: 0.494 | TrainAcc: 0.86, ValAcc: 0.84\n",
            "[51/200] TrainLoss: 0.408, ValLoss: 0.600 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[52/200] TrainLoss: 0.398, ValLoss: 0.608 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[53/200] TrainLoss: 0.401, ValLoss: 0.592 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[54/200] TrainLoss: 0.397, ValLoss: 0.599 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[55/200] TrainLoss: 0.396, ValLoss: 0.590 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[56/200] TrainLoss: 0.395, ValLoss: 0.568 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[57/200] TrainLoss: 0.394, ValLoss: 0.598 | TrainAcc: 0.86, ValAcc: 0.81\n",
            "[58/200] TrainLoss: 0.389, ValLoss: 0.556 | TrainAcc: 0.87, ValAcc: 0.82\n",
            "[59/200] TrainLoss: 0.397, ValLoss: 0.477 | TrainAcc: 0.86, ValAcc: 0.84\n",
            "[60/200] TrainLoss: 0.383, ValLoss: 0.521 | TrainAcc: 0.87, ValAcc: 0.83\n",
            "[61/200] TrainLoss: 0.387, ValLoss: 0.494 | TrainAcc: 0.87, ValAcc: 0.83\n",
            "[62/200] TrainLoss: 0.385, ValLoss: 0.456 | TrainAcc: 0.87, ValAcc: 0.85\n",
            "[63/200] TrainLoss: 0.376, ValLoss: 0.404 | TrainAcc: 0.87, ValAcc: 0.86\n",
            "[64/200] TrainLoss: 0.376, ValLoss: 0.504 | TrainAcc: 0.87, ValAcc: 0.83\n",
            "[65/200] TrainLoss: 0.377, ValLoss: 0.500 | TrainAcc: 0.87, ValAcc: 0.84\n",
            "[66/200] TrainLoss: 0.378, ValLoss: 0.560 | TrainAcc: 0.87, ValAcc: 0.82\n",
            "[67/200] TrainLoss: 0.372, ValLoss: 0.577 | TrainAcc: 0.87, ValAcc: 0.81\n",
            "[68/200] TrainLoss: 0.372, ValLoss: 0.475 | TrainAcc: 0.87, ValAcc: 0.84\n",
            "[69/200] TrainLoss: 0.363, ValLoss: 0.441 | TrainAcc: 0.87, ValAcc: 0.85\n",
            "[70/200] TrainLoss: 0.369, ValLoss: 0.437 | TrainAcc: 0.87, ValAcc: 0.85\n",
            "[71/200] TrainLoss: 0.360, ValLoss: 0.552 | TrainAcc: 0.88, ValAcc: 0.82\n",
            "[72/200] TrainLoss: 0.357, ValLoss: 0.450 | TrainAcc: 0.88, ValAcc: 0.85\n",
            "[73/200] TrainLoss: 0.356, ValLoss: 0.499 | TrainAcc: 0.88, ValAcc: 0.84\n",
            "[74/200] TrainLoss: 0.355, ValLoss: 0.377 | TrainAcc: 0.88, ValAcc: 0.87\n",
            "[75/200] TrainLoss: 0.357, ValLoss: 0.374 | TrainAcc: 0.88, ValAcc: 0.87\n",
            "[76/200] TrainLoss: 0.353, ValLoss: 0.559 | TrainAcc: 0.88, ValAcc: 0.82\n",
            "[77/200] TrainLoss: 0.355, ValLoss: 0.530 | TrainAcc: 0.88, ValAcc: 0.83\n",
            "[78/200] TrainLoss: 0.354, ValLoss: 0.587 | TrainAcc: 0.88, ValAcc: 0.82\n",
            "[79/200] TrainLoss: 0.346, ValLoss: 0.457 | TrainAcc: 0.88, ValAcc: 0.84\n",
            "[80/200] TrainLoss: 0.343, ValLoss: 0.422 | TrainAcc: 0.88, ValAcc: 0.86\n",
            "[81/200] TrainLoss: 0.344, ValLoss: 0.425 | TrainAcc: 0.88, ValAcc: 0.85\n",
            "[82/200] TrainLoss: 0.349, ValLoss: 0.402 | TrainAcc: 0.88, ValAcc: 0.86\n",
            "[83/200] TrainLoss: 0.337, ValLoss: 0.456 | TrainAcc: 0.88, ValAcc: 0.85\n",
            "[84/200] TrainLoss: 0.334, ValLoss: 0.470 | TrainAcc: 0.89, ValAcc: 0.84\n",
            "[85/200] TrainLoss: 0.339, ValLoss: 0.438 | TrainAcc: 0.88, ValAcc: 0.84\n",
            "[86/200] TrainLoss: 0.332, ValLoss: 0.428 | TrainAcc: 0.89, ValAcc: 0.86\n",
            "[87/200] TrainLoss: 0.326, ValLoss: 0.434 | TrainAcc: 0.89, ValAcc: 0.86\n",
            "[88/200] TrainLoss: 0.325, ValLoss: 0.431 | TrainAcc: 0.89, ValAcc: 0.85\n",
            "[89/200] TrainLoss: 0.324, ValLoss: 0.543 | TrainAcc: 0.89, ValAcc: 0.83\n",
            "[90/200] TrainLoss: 0.320, ValLoss: 0.371 | TrainAcc: 0.89, ValAcc: 0.86\n",
            "[91/200] TrainLoss: 0.314, ValLoss: 0.394 | TrainAcc: 0.89, ValAcc: 0.86\n",
            "[92/200] TrainLoss: 0.319, ValLoss: 0.481 | TrainAcc: 0.89, ValAcc: 0.84\n",
            "[93/200] TrainLoss: 0.318, ValLoss: 0.450 | TrainAcc: 0.89, ValAcc: 0.85\n",
            "[94/200] TrainLoss: 0.312, ValLoss: 0.485 | TrainAcc: 0.89, ValAcc: 0.85\n",
            "[95/200] TrainLoss: 0.315, ValLoss: 0.462 | TrainAcc: 0.89, ValAcc: 0.84\n",
            "[96/200] TrainLoss: 0.306, ValLoss: 0.421 | TrainAcc: 0.90, ValAcc: 0.87\n",
            "[97/200] TrainLoss: 0.298, ValLoss: 0.384 | TrainAcc: 0.90, ValAcc: 0.87\n",
            "[98/200] TrainLoss: 0.299, ValLoss: 0.377 | TrainAcc: 0.90, ValAcc: 0.87\n",
            "[99/200] TrainLoss: 0.301, ValLoss: 0.475 | TrainAcc: 0.90, ValAcc: 0.84\n",
            "[100/200] TrainLoss: 0.296, ValLoss: 0.387 | TrainAcc: 0.90, ValAcc: 0.87\n",
            "[101/200] TrainLoss: 0.293, ValLoss: 0.392 | TrainAcc: 0.90, ValAcc: 0.88\n",
            "[102/200] TrainLoss: 0.293, ValLoss: 0.357 | TrainAcc: 0.90, ValAcc: 0.88\n",
            "[103/200] TrainLoss: 0.286, ValLoss: 0.436 | TrainAcc: 0.90, ValAcc: 0.86\n",
            "[104/200] TrainLoss: 0.284, ValLoss: 0.429 | TrainAcc: 0.90, ValAcc: 0.87\n",
            "[105/200] TrainLoss: 0.280, ValLoss: 0.484 | TrainAcc: 0.90, ValAcc: 0.85\n",
            "[106/200] TrainLoss: 0.275, ValLoss: 0.370 | TrainAcc: 0.90, ValAcc: 0.87\n",
            "[107/200] TrainLoss: 0.279, ValLoss: 0.341 | TrainAcc: 0.90, ValAcc: 0.88\n",
            "[108/200] TrainLoss: 0.268, ValLoss: 0.354 | TrainAcc: 0.91, ValAcc: 0.88\n",
            "[109/200] TrainLoss: 0.272, ValLoss: 0.428 | TrainAcc: 0.91, ValAcc: 0.86\n",
            "[110/200] TrainLoss: 0.266, ValLoss: 0.354 | TrainAcc: 0.91, ValAcc: 0.89\n",
            "[111/200] TrainLoss: 0.266, ValLoss: 0.341 | TrainAcc: 0.91, ValAcc: 0.89\n",
            "[112/200] TrainLoss: 0.265, ValLoss: 0.330 | TrainAcc: 0.91, ValAcc: 0.89\n",
            "[113/200] TrainLoss: 0.259, ValLoss: 0.379 | TrainAcc: 0.91, ValAcc: 0.88\n",
            "[114/200] TrainLoss: 0.252, ValLoss: 0.351 | TrainAcc: 0.91, ValAcc: 0.88\n",
            "[115/200] TrainLoss: 0.252, ValLoss: 0.345 | TrainAcc: 0.91, ValAcc: 0.89\n",
            "[116/200] TrainLoss: 0.253, ValLoss: 0.391 | TrainAcc: 0.91, ValAcc: 0.87\n",
            "[117/200] TrainLoss: 0.247, ValLoss: 0.309 | TrainAcc: 0.91, ValAcc: 0.89\n",
            "[118/200] TrainLoss: 0.244, ValLoss: 0.426 | TrainAcc: 0.92, ValAcc: 0.86\n",
            "[119/200] TrainLoss: 0.238, ValLoss: 0.342 | TrainAcc: 0.92, ValAcc: 0.88\n",
            "[120/200] TrainLoss: 0.239, ValLoss: 0.401 | TrainAcc: 0.92, ValAcc: 0.87\n",
            "[121/200] TrainLoss: 0.234, ValLoss: 0.341 | TrainAcc: 0.92, ValAcc: 0.89\n",
            "[122/200] TrainLoss: 0.228, ValLoss: 0.387 | TrainAcc: 0.92, ValAcc: 0.87\n",
            "[123/200] TrainLoss: 0.224, ValLoss: 0.349 | TrainAcc: 0.92, ValAcc: 0.88\n",
            "[124/200] TrainLoss: 0.226, ValLoss: 0.333 | TrainAcc: 0.92, ValAcc: 0.89\n",
            "[125/200] TrainLoss: 0.213, ValLoss: 0.381 | TrainAcc: 0.93, ValAcc: 0.89\n",
            "[126/200] TrainLoss: 0.219, ValLoss: 0.339 | TrainAcc: 0.92, ValAcc: 0.89\n",
            "[127/200] TrainLoss: 0.215, ValLoss: 0.316 | TrainAcc: 0.93, ValAcc: 0.89\n",
            "[128/200] TrainLoss: 0.211, ValLoss: 0.342 | TrainAcc: 0.93, ValAcc: 0.88\n",
            "[129/200] TrainLoss: 0.208, ValLoss: 0.309 | TrainAcc: 0.93, ValAcc: 0.90\n",
            "[130/200] TrainLoss: 0.200, ValLoss: 0.362 | TrainAcc: 0.93, ValAcc: 0.88\n",
            "[131/200] TrainLoss: 0.196, ValLoss: 0.344 | TrainAcc: 0.93, ValAcc: 0.89\n",
            "[132/200] TrainLoss: 0.196, ValLoss: 0.363 | TrainAcc: 0.93, ValAcc: 0.89\n",
            "[133/200] TrainLoss: 0.190, ValLoss: 0.363 | TrainAcc: 0.93, ValAcc: 0.88\n",
            "[134/200] TrainLoss: 0.184, ValLoss: 0.318 | TrainAcc: 0.94, ValAcc: 0.90\n",
            "[135/200] TrainLoss: 0.184, ValLoss: 0.363 | TrainAcc: 0.94, ValAcc: 0.90\n",
            "[136/200] TrainLoss: 0.185, ValLoss: 0.307 | TrainAcc: 0.94, ValAcc: 0.90\n",
            "[137/200] TrainLoss: 0.177, ValLoss: 0.331 | TrainAcc: 0.94, ValAcc: 0.90\n",
            "[138/200] TrainLoss: 0.178, ValLoss: 0.339 | TrainAcc: 0.94, ValAcc: 0.90\n",
            "[139/200] TrainLoss: 0.170, ValLoss: 0.325 | TrainAcc: 0.94, ValAcc: 0.89\n",
            "[140/200] TrainLoss: 0.166, ValLoss: 0.300 | TrainAcc: 0.94, ValAcc: 0.91\n",
            "[141/200] TrainLoss: 0.159, ValLoss: 0.284 | TrainAcc: 0.94, ValAcc: 0.91\n",
            "[142/200] TrainLoss: 0.156, ValLoss: 0.286 | TrainAcc: 0.95, ValAcc: 0.91\n",
            "[143/200] TrainLoss: 0.155, ValLoss: 0.298 | TrainAcc: 0.95, ValAcc: 0.91\n",
            "[144/200] TrainLoss: 0.144, ValLoss: 0.298 | TrainAcc: 0.95, ValAcc: 0.91\n",
            "[145/200] TrainLoss: 0.145, ValLoss: 0.373 | TrainAcc: 0.95, ValAcc: 0.89\n",
            "[146/200] TrainLoss: 0.142, ValLoss: 0.387 | TrainAcc: 0.95, ValAcc: 0.89\n",
            "[147/200] TrainLoss: 0.136, ValLoss: 0.292 | TrainAcc: 0.95, ValAcc: 0.91\n",
            "[148/200] TrainLoss: 0.131, ValLoss: 0.286 | TrainAcc: 0.95, ValAcc: 0.91\n",
            "[149/200] TrainLoss: 0.132, ValLoss: 0.295 | TrainAcc: 0.95, ValAcc: 0.91\n",
            "[150/200] TrainLoss: 0.120, ValLoss: 0.301 | TrainAcc: 0.96, ValAcc: 0.91\n",
            "[151/200] TrainLoss: 0.117, ValLoss: 0.297 | TrainAcc: 0.96, ValAcc: 0.91\n",
            "[152/200] TrainLoss: 0.118, ValLoss: 0.287 | TrainAcc: 0.96, ValAcc: 0.91\n",
            "[153/200] TrainLoss: 0.110, ValLoss: 0.308 | TrainAcc: 0.96, ValAcc: 0.90\n",
            "[154/200] TrainLoss: 0.106, ValLoss: 0.299 | TrainAcc: 0.96, ValAcc: 0.91\n",
            "[155/200] TrainLoss: 0.105, ValLoss: 0.269 | TrainAcc: 0.96, ValAcc: 0.92\n",
            "[156/200] TrainLoss: 0.095, ValLoss: 0.322 | TrainAcc: 0.97, ValAcc: 0.91\n",
            "[157/200] TrainLoss: 0.099, ValLoss: 0.328 | TrainAcc: 0.97, ValAcc: 0.90\n",
            "[158/200] TrainLoss: 0.094, ValLoss: 0.265 | TrainAcc: 0.97, ValAcc: 0.92\n",
            "[159/200] TrainLoss: 0.087, ValLoss: 0.324 | TrainAcc: 0.97, ValAcc: 0.91\n",
            "[160/200] TrainLoss: 0.083, ValLoss: 0.278 | TrainAcc: 0.97, ValAcc: 0.92\n",
            "[161/200] TrainLoss: 0.079, ValLoss: 0.302 | TrainAcc: 0.97, ValAcc: 0.91\n",
            "[162/200] TrainLoss: 0.076, ValLoss: 0.327 | TrainAcc: 0.97, ValAcc: 0.91\n",
            "[163/200] TrainLoss: 0.070, ValLoss: 0.344 | TrainAcc: 0.98, ValAcc: 0.91\n",
            "[164/200] TrainLoss: 0.069, ValLoss: 0.294 | TrainAcc: 0.98, ValAcc: 0.92\n",
            "[165/200] TrainLoss: 0.064, ValLoss: 0.282 | TrainAcc: 0.98, ValAcc: 0.92\n",
            "[166/200] TrainLoss: 0.060, ValLoss: 0.289 | TrainAcc: 0.98, ValAcc: 0.92\n",
            "[167/200] TrainLoss: 0.053, ValLoss: 0.260 | TrainAcc: 0.98, ValAcc: 0.93\n",
            "[168/200] TrainLoss: 0.052, ValLoss: 0.294 | TrainAcc: 0.98, ValAcc: 0.92\n",
            "[169/200] TrainLoss: 0.053, ValLoss: 0.255 | TrainAcc: 0.98, ValAcc: 0.93\n",
            "[170/200] TrainLoss: 0.046, ValLoss: 0.266 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[171/200] TrainLoss: 0.044, ValLoss: 0.273 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[172/200] TrainLoss: 0.043, ValLoss: 0.270 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[173/200] TrainLoss: 0.036, ValLoss: 0.279 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[174/200] TrainLoss: 0.036, ValLoss: 0.253 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[175/200] TrainLoss: 0.033, ValLoss: 0.267 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[176/200] TrainLoss: 0.030, ValLoss: 0.269 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[177/200] TrainLoss: 0.029, ValLoss: 0.260 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[178/200] TrainLoss: 0.027, ValLoss: 0.267 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[179/200] TrainLoss: 0.025, ValLoss: 0.253 | TrainAcc: 0.99, ValAcc: 0.94\n",
            "[180/200] TrainLoss: 0.024, ValLoss: 0.248 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[181/200] TrainLoss: 0.022, ValLoss: 0.262 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[182/200] TrainLoss: 0.021, ValLoss: 0.264 | TrainAcc: 0.99, ValAcc: 0.94\n",
            "[183/200] TrainLoss: 0.021, ValLoss: 0.258 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[184/200] TrainLoss: 0.019, ValLoss: 0.251 | TrainAcc: 0.99, ValAcc: 0.93\n",
            "[185/200] TrainLoss: 0.017, ValLoss: 0.249 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[186/200] TrainLoss: 0.018, ValLoss: 0.253 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[187/200] TrainLoss: 0.018, ValLoss: 0.262 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[188/200] TrainLoss: 0.016, ValLoss: 0.255 | TrainAcc: 1.00, ValAcc: 0.93\n",
            "[189/200] TrainLoss: 0.015, ValLoss: 0.260 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[190/200] TrainLoss: 0.015, ValLoss: 0.256 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[191/200] TrainLoss: 0.013, ValLoss: 0.251 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[192/200] TrainLoss: 0.014, ValLoss: 0.252 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[193/200] TrainLoss: 0.014, ValLoss: 0.252 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[194/200] TrainLoss: 0.014, ValLoss: 0.252 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[195/200] TrainLoss: 0.013, ValLoss: 0.247 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[196/200] TrainLoss: 0.014, ValLoss: 0.261 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[197/200] TrainLoss: 0.014, ValLoss: 0.248 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[198/200] TrainLoss: 0.014, ValLoss: 0.253 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[199/200] TrainLoss: 0.014, ValLoss: 0.249 | TrainAcc: 1.00, ValAcc: 0.94\n",
            "[200/200] TrainLoss: 0.013, ValLoss: 0.252 | TrainAcc: 1.00, ValAcc: 0.94\n"
          ]
        }
      ],
      "source": [
        "epochs = 200\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # since pytorch add gradients, initialize to 0 for each iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)        \n",
        "\n",
        "        # back propagate and update parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_correct += predicted.eq(labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "    train_loss = train_loss / train_batches\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss = val_loss / val_batches\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    # save the best model (scored by validation loss)\n",
        "    if val_loss < best_valid_loss:\n",
        "        torch.save(model.state_dict(), './2018312292_DongwonKim.pt')\n",
        "        best_valid_loss = val_loss\n",
        "    \n",
        "    print('[%d/%d] TrainLoss: %.3f, ValLoss: %.3f | TrainAcc: %.2f, ValAcc: %.2f'\\\n",
        "          % (epoch+1, epochs, train_loss, val_loss, train_acc, val_acc))\n",
        "    \n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUhXWEKsMAsQ"
      },
      "source": [
        "# Download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "otyrxXhVjmSx",
        "outputId": "c26f97f2-9205-4c2f-f28b-b65e32a8ee46"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f17c4752-bc16-401f-b6ca-16812a50855d\", \"2018312292_DongwonKim.pt\", 4358557)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download('./DenseNet_CIFAR10.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JB9kktkMEhW"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCBA3DvaMFtq"
      },
      "source": [
        "## load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMSEkPFvFDEU"
      },
      "outputs": [],
      "source": [
        "new_model = DenseNet(droprate=0.2)\n",
        "new_model.load_state_dict(torch.load('./DenseNet_CIFAR10.pt'))\n",
        "new_model = new_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICF25FqMQF0"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgEGDh-vFXfX",
        "outputId": "55dd895d-0034-4803-8bf1-a43309b825b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TestAcc: 0.93\n"
          ]
        }
      ],
      "source": [
        "test_loss = 0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_batches = len(test_loader)\n",
        "\n",
        "new_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = new_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        test_correct += predicted.eq(labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "test_loss = test_loss / test_batches\n",
        "test_acc = test_correct / test_total\n",
        "\n",
        "print('TestAcc: %.2f' % (test_acc))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "2018312292_DongwonKim.ipynb의 사본의 사본",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
